{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import inception_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setup gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = './data/train.csv'\n",
    "TRAIN_IMAGES_DIR = './data/train_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load csv using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scab                               4826\n",
       "healthy                            4624\n",
       "frog_eye_leaf_spot                 3181\n",
       "rust                               1860\n",
       "complex                            1602\n",
       "powdery_mildew                     1184\n",
       "scab frog_eye_leaf_spot             686\n",
       "scab frog_eye_leaf_spot complex     200\n",
       "frog_eye_leaf_spot complex          165\n",
       "rust frog_eye_leaf_spot             120\n",
       "rust complex                         97\n",
       "powdery_mildew complex               87\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_csv(TRAIN_CSV, dtype=str)\n",
    "#traindf['labels'] = traindf['labels'].str.split()\n",
    "traindf['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load images using ImageDataGenerator, copied from [StackOverflow](https://stackoverflow.com/questions/59464409/loading-images-in-keras-for-cnn-from-directory-but-label-in-csv-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14906 validated image filenames belonging to 12 classes.\n",
      "Found 3726 validated image filenames belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "shape = (256, 256, 3)\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255., validation_split=0.2)\n",
    "train_generator = datagen.flow_from_dataframe(dataframe = traindf,\n",
    "                                              directory = TRAIN_IMAGES_DIR,\n",
    "                                              featurewise_std_normalization = True,\n",
    "                                              x_col = 'image',\n",
    "                                              y_col = 'labels',\n",
    "                                              subset = 'training',\n",
    "                                              batch_size = 32,\n",
    "                                              seed = 1,\n",
    "                                              shuffle = True,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              target_size = shape[:2]\n",
    "                                             )\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(dataframe = traindf,\n",
    "                                              directory = TRAIN_IMAGES_DIR,\n",
    "                                              featurewise_std_normalization = True,\n",
    "                                              x_col = 'image',\n",
    "                                              y_col = 'labels',\n",
    "                                              subset = 'validation',\n",
    "                                              batch_size = 32,\n",
    "                                              seed = 1,\n",
    "                                              shuffle = True,\n",
    "                                              class_mode = 'categorical',\n",
    "                                              target_size = shape[:2]\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(iter(train_generator))\n",
    "val_images, val_labels = next(iter(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# classes_amount = len(set(train_generator.classes))\n",
    "classes_amount = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 196608)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25165952  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                1548      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,167,500\n",
      "Trainable params: 25,167,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 21:56:22.254525: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "  [\n",
    "    keras.Input(shape=train_generator.image_shape),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes), # now the size of the last layer is 12, not 14906, so everything fits\n",
    "  ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',#keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_generator, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_13  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imagenet = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in imagenet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    keras.Input(shape = train_generator.image_shape),\n",
    "    imagenet,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model1.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Adam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "466/466 [==============================] - 1610s 3s/step - loss: 1.2694 - accuracy: 0.5896\n",
      "Epoch 2/5\n",
      "466/466 [==============================] - 1594s 3s/step - loss: 1.0445 - accuracy: 0.6617\n",
      "Epoch 3/5\n",
      "466/466 [==============================] - 1596s 3s/step - loss: 0.9705 - accuracy: 0.6821\n",
      "Epoch 4/5\n",
      "466/466 [==============================] - 1620s 3s/step - loss: 0.9219 - accuracy: 0.6956\n",
      "Epoch 5/5\n",
      "466/466 [==============================] - 1604s 3s/step - loss: 0.8876 - accuracy: 0.7086\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(train_generator, \n",
    "                     epochs = 5,\n",
    "                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 00:19:51.771702: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/m1/assets\n"
     ]
    }
   ],
   "source": [
    "model1.save(f\"model/m1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_predictions = model1.predict(val_images)\n",
    "\n",
    "y_true = [np.argmax(row) for row in val_labels]\n",
    "y_pred = [np.argmax(row) for row in model_predictions]\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.imshow(matrix)\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "30/30 [==============================] - 102s 3s/step - loss: 2.4001 - accuracy: 0.1611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.400103807449341, 'accuracy': 0.1611170768737793}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate\")\n",
    "result = model1.evaluate(validation_generator)\n",
    "dict(zip(model1.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_32 (Sequential)  (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_19  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intercept_layer = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in intercept_layer.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "imput_layer = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "    \n",
    "model2 = keras.Sequential([\n",
    "    imput_layer,\n",
    "    intercept_layer,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model2.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Adam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 1599s 5s/step - loss: 1.4324 - accuracy: 0.5279 - val_loss: 1.0670 - val_accuracy: 0.5938\n",
      "INFO:tensorflow:Assets written to: model/m2/assets\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(train_generator, \n",
    "                     epochs = 1,\n",
    "                     validation_data = validation_data,\n",
    "                     verbose = 1)\n",
    "\n",
    "model2.save(f\"model/m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "292/292 [==============================] - 2343s 8s/step - loss: 1.2721 - accuracy: 0.5989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.272117257118225, 'accuracy': 0.5988621711730957}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate\")\n",
    "\n",
    "result = model2.evaluate(validation_generator)\n",
    "dict(zip(model2.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f80b61f5130>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_predictions2 = model2.predict(val_images)\n",
    "\n",
    "y_true = [np.argmax(row) for row in val_labels]\n",
    "y_pred2 = [np.argmax(row) for row in model_predictions2]\n",
    "\n",
    "matrix = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred2))\n",
    "\n",
    "matrix.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_2 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intercept_layer = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in intercept_layer.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "imput_layer = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "    \n",
    "model4 = keras.Sequential([\n",
    "    imput_layer,\n",
    "    intercept_layer,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model4.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Adam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "466/466 [==============================] - 2654s 6s/step - loss: 1.3300 - accuracy: 0.5694 - val_loss: 1.2444 - val_accuracy: 0.5312\n",
      "Epoch 2/5\n",
      "466/466 [==============================] - 1618s 3s/step - loss: 1.1447 - accuracy: 0.6299 - val_loss: 1.1442 - val_accuracy: 0.5312\n",
      "Epoch 3/5\n",
      "466/466 [==============================] - 1613s 3s/step - loss: 1.1033 - accuracy: 0.6385 - val_loss: 1.1285 - val_accuracy: 0.5625\n",
      "Epoch 4/5\n",
      "466/466 [==============================] - 1612s 3s/step - loss: 1.0679 - accuracy: 0.6499 - val_loss: 1.0350 - val_accuracy: 0.5312\n",
      "Epoch 5/5\n",
      "466/466 [==============================] - 1615s 3s/step - loss: 1.0509 - accuracy: 0.6573 - val_loss: 1.0763 - val_accuracy: 0.6250\n",
      "INFO:tensorflow:Assets written to: model/m4/assets\n"
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    verbose=1)\n",
    "\n",
    "model4.save(f\"model/m4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate\n",
      "117/117 [==============================] - 397s 3s/step - loss: 1.0149 - accuracy: 0.6669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.0149343013763428, 'accuracy': 0.6669350266456604}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluate\")\n",
    "\n",
    "result = model4.evaluate(validation_generator)\n",
    "dict(zip(model4.metrics_names, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f9f2dced820>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEKCAYAAACoiGheAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhdklEQVR4nO3de7hVVb3/8fdnbdjc5CJxCWErEIY/UkQOD17zQTmVWkfLp0yzTpeTaFqJZRdLs8uTnd/5/X4d61ieOJqFCh0xvJWXramJF1QgUhRNxQsXEdH2AUG57P39/THn1i2y15qLNeZaayy+r+eZD2utPed3jDX3djjmmGOOr8wM55yLQaHWFXDOuay8wXLORcMbLOdcNLzBcs5Fwxss51w0vMFyzkXDGyznXE1IGi9paZdtg6SZRY/xeVjOuVqT1ASsBg42s+e72897WM65ejAdeKZYYwXQo0qVyaRZvaw3/XKLv3VEfrEBml/clGt8V5r69M69DHv9jdzLyMsbbGKrbVElMT50VD975dX2TPsufmTLY0DXEzbLzGbtZNeTgbml4tVVg9Wbfhys6bnFX3naYbnFBmj50f25xnelFfabkHsZHUsfz72MvDxof6o4xvpX23nwtlGZ9u054pk3zGxKsX0kNQPHA+eVildXDZZzLgZGu3WEDHgssMTMXiq1ozdYzrmyGNBB0Jt1p5DhchC8wXLO7YIOwvSwJPUDPgCcnmV/b7Ccc2UxjG2BLgnNbBPwrqz7e4PlnCuLAe1hLwkz8wbLOVe2wGNYmUXZYE2ZtoEzfrSGpoJxy9zBXHPJ8GCxm5u2M/uEG2huaqdHoYPWFWO55OGpweJDvvWvVhmxxz9n5kKmTl1DW1tvvnTmcUFjQ/znpxgD2mv0hEyuM90lHSPpSUlPS/p2iJiFgnHWRas5/9QxnDZtPEed0Mbe+4abyLe1vYkv3Hg8J847iRPnfYIjWlYycfjaYPHzrn81yog9PsDtd4zl/AumBY3ZqRHOTykdGbfQcmuw0meDfkEyx2ICcIqkimf1jT9oM2uea2btC73Yvq3A3TcM4tAP/U+lYbsQm7f3BKBHoYMehQ6wiiYGv03+9c+/jNjjAyxbNoyNG5uDxuzUCOenGMNoz7iFlmcPayrwtJmtMLOtwO+AEyoN+q53b+PlNW/9oa1/sSdDRmyrNOzbFNTB/E9cw72f+w33rxrFI+vCdberUf+8y4g9ft4a/fyYwbaMW2h5NlgjgZVd3q9KP3sbSTMkLZK0aBtbcqxOdh1W4MR5J3HU7H/mgGHrGDf4lVpXybk6ItozbqHVfLUGM5tlZlPMbEpPepXc/5W1PRm619Y33w8ZsY31L/bMpW4bt/biodUjeX/LytI7Z1SN+uddRuzx89bo58eADsu2hZZng7UaaOnyflT6WUWeXNqXkWO2MrxlCz16djDthDYWtg6sNOyb9uz9Ov2bk55er6btHNaykhVtg4LFz7v+1Sgj9vh52x3OT616WHlOa3gY2FfSGJKG6mTgU5UG7WgXv/juSC6as4JCE7T+bjDP/y3ckiJD+27mJ0ffSaHQQUHGrU+P48/Pjw4WP+/6V6OM2OMDfOub9zFx4joGDNjClbOv58qrDqC19T1BYjfC+SkmmTgavjHKItcVRyUdB1wMNAG/NrMfF9t/gAZbrsvLXODLyzS6wiRfXqaYB+1PbLBXK2ptJkxstqv+8O5M+/7DPisXl1pephy5Thw1s5uBm/MswzlXXYZor9Hwd5Qz3Z1ztdURcG5iObzBcs6VpZZjWN5gOefKJNrNLwmdcxFIVhz1Bss5FwEzsdWaalK2N1jOubJ1+BhW/va5qS3fAnwOUEl5z5OK/fzEIBl090tC51wUfNDdORcJH3R3zkWl3SeOOudiYIhtFqbpkDQIuAzYn6Tz9gUze6C7/b3Bcs6VJfCg+8+AW83s45Kagb7FdvYGyzlXFkNBLgklDQSOBD4HkC6lvrXYMTVfcdQ5F58OCpk2YEjnEujpNqNLmDHAy8AVkv4i6bI0dX23ouxhxZ6zLu/4EHdePD8/tY9fjBnlTGtYX2Q9rB7AZOArZvagpJ8B3wYu6C5Ynmm+fi1pnaRlIePGnrOuGvFjz4vn56e28UtJBt2bMm0lrAJWmdmD6ftrSRqwbuV5Sfgb4JjQQWPPWVeN+LHnxfPzU9v4WbRTyLQVY2ZrgZWSxqcfTQeKPqqQW4NlZvcAr4aOW+ucbDFo9Lx4lYr9/NT6/Buiw7JtGXwFuFrSI8Ak4KJiO0c5huWcq61Q0xrMbCmQec33mjdY6V2DGQC9i0/BAGqfky0GjZ4Xr1Kxn59an/8kL2FtJhjUfFpDuYlU6yEnW73bHfLiVSL281P781+7zM8172GVK/acddWIH3tePD8/tY1fikGWO4C5yC0voaS5wDRgCPAScKGZXV7smLzzElYjZ13eYl/vydfDqq0QeQlHvm+QnXnNEZn2PX//P8aRl9DMTskrtnOutnw9LOdcFJL1sHx5GedcFHzFUedcJJJpDd7Dcs5FoPNZwlrwBss5VzZf0905F4VkeRm/JER9elPYL965UrfcPCf3Mj6016Tcy8iTz5NqDD6G5ZyLQrJag18SOucikDya4w2Wcy4K3sNyzkXEZ7o756Lgdwmdc1HxS0LnXBQ613SvBW+wnHNlMWC797Cyiz3R6cqne3HRGaPffL/2hWY+8421nHjay8HKiD2Rp8evbfxSGm5Nd0ktku6S9LikxySdHSp27IlOW8Zt4dI7nuTSO57kktuepFefDg4/ti1Y/NgTeXr82sYvKWOKrzwuG/NsJrcDXzezCcAhwFmSgjx3E3ui066WLujPiH22MHxUuLxysSfy9Pi1jV9K5wJ+WbZSJD0n6VFJSyUtKrV/nolUXzSzJenrjcByYGRe5cXq7hsGMe2jbUFjxp7I0+PXNn4WgXtYR5nZpCxrv1flQlTSaOAg4MGd/GyGpEWSFm3dvrka1akb27aKha0DOfKf2mpdFecy61zArxaXhLkPukvaA/g9MNPMNuz4czObBcwCGNh3r3xS+NSph+/sz7gDNrPn0O1B48aeyNPj1zZ+KYbY3pG5rzNkh0u9Wel/82+Fg1ZJBvxqh5+9Q649LEk9SRqrq81sfp5lxeju6/cMfjkI8Sfy9Pi1jZ9FGWNY6zsTJafbjg3SEWY2GTiWZJz7yGLl5tbDkiTgcmC5mf00ZOzYE50CvLG5wJIF/Tn731YGjQvxJ/L0+LWNX5KFWw/LzFan/66TdB0wFbinu/3zTKR6BLAAeBToSD/+jpnd3N0xA/vuZYfsd1ou9akGX8DP1bsQiVQHjB9uB//qU5n2veOoi7tNpCqpH1Aws43p69uBH5rZrd3FyzOR6r1Qo0e6nXO5CtTDGg5cl1yM0QOYU6yx6tzJOecyM0R79kH37uOYrQAOLOcYb7Ccc2Xz9bCcc1GwgIPu5fIGyzlXNvMGyzkXB18PyzkXEe9hAfb6G1En2qzGHKmVFxyWa/yWH92fa3wXPzNo7/AGyzkXCb9L6JyLguGXhM65aPigu3MuIjk9glySN1jOubL5JaFzLgrJXUJP8+Wci4RfEpYh9pxvecdvbtrO7BNuoLmpnR6FDlpXjOWSh6cGLSP2c+TxK1OrS8I88xL2lvSQpL+meQl/ECJu7DnfqpFTbmt7E1+48XhOnHcSJ877BEe0rGTi8LXB4sd+jjx+ZQxhlm0LLc8L0S3A0WZ2IDAJOEbSIZUGjT3nW3VyyonN25OkBD0KHfQodEDAP57Yz5HHr5xl3ELLMy+hmdlr6due6Vbxd4g951u1csoV1MH8T1zDvZ/7DfevGsUj68JdMsR+jjx+hQysQ5m20PLOmtMkaSmwDrjdzIrmJdzGljyrs1vpsAInzjuJo2b/MwcMW8e4wa/UukqugTTiJSFm1m5mk4BRwFRJ++9kn1mdKYB60qtkzNhzvlU7p9zGrb14aPVI3t8SLjtP7OfI41fOLNsWWrcNlqT/kPTz7rZyCjGzNuAu4JgK6xt9zrdq5JTbs/fr9G9Oequ9mrZzWMtKVrQNChY/9nPk8SvT+SxhLXpYxaY1LCrys5IkDQW2mVmbpD7AB4D/XUlMiD/nWzVyyg3tu5mfHH0nhUIHBRm3Pj2OPz8/Olj82M+Rx6+QEfQmTjky5yWU1NfMNmcOLE0Efgs0kfTkrjGzHxY7ZoAG28GanrWI3ZKvh+UqESIvYa+xI23kj8/KtO+zn/put3kJd0XJiaOSDiXJ4LwHsLekA4HTzezMYseZ2SPAQUFq6ZyrI2HvAEpqIrmiW21mHym2b5ZB94uBDwGvAJjZX4EjK6yjcy5mYSdinQ0sz7JjpruEZrbjLab2zFVxzjUWCzfoLmkU8GHgsixFZ3mWcKWkwwCT1JMyWkPnXIMKN2XhYuCbQP8sO2fpYZ0BnAWMBNaQPGaTbcTNOdeglHFjSOfE8HSb8WYE6SPAOjNbnLXUkj0sM1sPnFrGN3HONbqOzHuuL3KX8HDgeEnHAb2BAZKuMrNPdxesZA9L0lhJN0l6WdI6STdIGpu5us65xtI5DyvLViyM2XlmNsrMRgMnA3cWa6wg2xjWHOAXwMfS9ycDc4GDMxzrAst7nlRh0oRc48ecd9K9pVYL+GUZw+prZlea2fZ0u4qk++ac210FXl/GzO4uNQcLivSwJA1OX94i6dvA79IqfBK4OXtVnHMNpw6TUCwmaaA6a3Z6l58ZcF5elXLO1TfV25ruZjammhVxzkXCBDkszpdFpiQU6TpWE+gydmVms/OqlHOuztVbD6uTpAuBaSQN1s3AscC9gDdYzu2u6vgu4ceB6cBaM/s8cCBQvdXCnHP1p0ZZKLI0WK+bWQewXdIAkvXZW8JXJbsp0zZw2YInuOK+5Zz05Zc8fpXLOGfmQubOmc+lv8zvZnHsv4PY4xcVaOLorsjSYC2SNAj4L5I7h0uAB7IWkCai+IukP+xaFd8u9pxv1cgpl3cZt98xlvMvmBYs3o5i/x3EHj8LWbYttJINlpmdaWZtZvafJMscfza9NMwq6OoOsed8q0ZOubzLWLZsGBs3NpfecRfF/juIPX4m9XZJKGnyjhswGOiRvi6p3LVusog951s1csrVPG9dhWL/HcQeP4ta9bCK3SX8f0V+ZsDRGeJfTIm1btLlJmYA9KZvhpDOuZqrt5nuZnZUJYG7rnUjaVqRcmYBsyBJQlEqbuw536qRU64e8tZVIvbfQezxS8orD30GeSZS7Vzr5jmS5xCPlnRVpUFjz/lWjZxytc5bV6nYfwexx8+kRmNYmWa67wozO4/0ecO0h3VuqbVusog951s1csrlXca3vnkfEyeuY8CALVw5+3quvOoAWlvfEyx+7L+D2ONnoewL+IUtN2tewooKeavBKrp8hOclrD1fD6uxBclL2NJio84+J9O+K77x9aB5CbOsOCpJn5b0vfT93pKmllNI1rVunHP1L+sdwprMwwJ+CRwKnJK+30iyAqlzbndVo5nuWcawDjazyZL+AmBmf5eU36xB51z9q9fVGoBtaSppA5A0lHJyZjjnGk7dLeDXxc+B64Bhkn5MsnrD+bnWyjlXv6x2dwmz5CW8WtJikiVmBHzUzDzzs3O7s3rtYUnaG9gM3NT1MzN7Ic+KOefqWL02WMAfeSsZRW9gDPAk8L4c6+VqxOdJuSxCjGFJ6g3cA/QiaYuuNbMLix2T5ZLwgB0KmQycWUE9nXMOYAtwtJm9JqkncK+kW8xsYXcHlP1ojpktkeRZn53bnQXoYVnymM1r6due6VY0cpYxrK91eVsAJgNrdrGOzrnYBbxLmE6ZWgyMA35hZg8W2z/LTPf+XbZeJGNaJ1RYT+dczLKv1jBE0qIu24y3hTFrN7NJwChgappSsFtFe1hp69ffzM7dle/knGs8oqxB9/VZHn42szZJdwHHAMu626/YEsk9zKydZF0r55x7S4D1sCQNTRPcIKkPSc6IJ4odU6yH9RDJeNVSSTcC84BNb9bXbH7x6jjnGlK4lRhGAL9Nr+QKwDVmVjS7Vpa7hL2BV0jWcO+cj2VAzRqsKdM2cMaP1tBUMG6ZO5hrLhnu8atchsdv7PglBRh0N7NHgIPKOabYoPuw9A7hMuDR9N/H0n+7vcbsStJzkh6VtFTSonIq1p3Yc741Ql5Cj9/Y8bOox/WwmoA90q1/l9edW1ZHmdmkUKsOxp7zrRHyEnr8xo6fSR2u6f6imf0wfJGV2VlOtv0mb/b4VSzD4zd2/JLqNGtOiOUCDWiVtHjH+RdvFiLN6JyjsY0tAYp0zuWtHhOphsgGcYSZrZY0DLhd0hNmdk/XHTwvYXx5CT1+Y8fPpN56WGb2aqXBzWx1+u86kkUAy0pesTOx53xrhLyEHr+x42ehjmxbaLnlJZTUDyiY2cb09QeBisfEYs/51gh5CT1+Y8cvqYZjWLnlJZQ0lqRXBUnDOMfMflzsGM9L6Fy+QuQl7Du8xcad+rXSOwKP/vvXguYlzDPz8wrgwLziO+dqqI5XHHXOubep56w5zjn3dt5gOeeiUM9pvpxz7h28h+Wci4WPYTnn4uENVv4KkybkGr9twoBc4wMMmNNtBiQH3LZmae5lfGivSbmXUe+8h+Wci4MRZAG/XeENlnOuLGUmoQjKGyznXPm8wXLOxUI5PYNcijdYzrny1HC1Bm+wnHNl8zEs51w0/NGcMuSdk+2cmQuZOnUNbW29+dKZxwWNDTBs4Gt875S7GNx/M2bihoX/i2vuPSBoGbHnxcsz/sqne3HRGaPffL/2hWY+8421nHjay8HKiPn8ZBKghyWpBZgNDE8jzjKznxU7JtcGK01DfRmwf1qhL5jZA5XE7MzJdt7JY1n/Yk/+4+anWHjbQF54KtyKi7ffMZYbb3ov5349n0ma7R3i5zcdwt9WD6Vvr61cMXM+Dz01iude2jNI/LzPUezxW8Zt4dI7ngSgvR1Onfw+Dj+2LUhsiP/8lBQuwcR24OtmtkRSf2CxpNvN7PHuDiiWNSeEnwG3mtl+JIv5La80YDVysi1bNoyNG5tL77iLXtnYj7+tHgrA5i3NPPfSIIYO2BQsfux58aqZd2/pgv6M2GcLw0dtCxazkc5PtwLkJTSzF81sSfp6I0n7MLLYMbk1WJIGAkcCl6cV2mpmbZXG3VlOtiEjwv2xVdu799zIe0e+wmMvDAsWM+9zFHv8ru6+YRDTPtoWNGYjnZ+d6Zw4mjHN15DONH7p1l26v9EkaesfLFZ2npeEY4CXgSskHQgsBs42s7d1JdIvMAOgN31zrE796dO8jZ98tpWLbziUzVvy69G5ndu2VSxsHcgXvvNirasSHXVkviZcX2pNd0l7AL8HZprZhmL75nlJ2AOYDFxqZgcBm4Bv77iTmc0ysylmNqUnvUoGrYucbAE0Fdq56LOt3LZkX/68bGzQ2LHnxavW7/jhO/sz7oDN7Dl0e9C4jXJ+upX1cjBDmyapJ0ljdbWZzS+1f54N1ipglZl1dvGuJWnAKlIPOdkqZ3z3pD/z/EuD+N09E4NHjz0vXrV+x3dfv2fwy0FonPNTTIi8hJJEMmS03Mx+mqXcPLPmrJW0UtJ4M3uSJJN0t6P/WVUjJ9u3vnkfEyeuY8CALVw5+3quvOoAWlvfEyz+xNFrOXbKUzy9ZjC/PedaAP7zlqk88MTeQeLHnhevGr/jNzYXWLKgP2f/28qgcaExzk9JYe4SHg58BnhU0tL0s++Y2c3dHZBbXkIASZNIpjU0AyuAz5vZ37vbP++8hL4eVuPz9bCKC5GXcI/BLXbg9JmZ9r3/2nPjyEsIYGZLgWCVdc7VAQP84WfnXCz80RznXBR8AT/nXDzM/JLQORcP72E55+LhDZZzLhbew3LOxcGAdh/Dyl3H0oon2hc1YGmu4V0G1ZjUmfcE5Lz/TkPwHpZzLh5+l9A5FwvvYTnn4uBpvpxzsRAgH3R3zsXCMz875+JQw0vCvLPm5GLKtA1ctuAJrrhvOSd9+SWPX4MyPH5x58xcyNw587n0l92uRVeRavwNdc/eep6w1BZYnllzxkta2mXbIGlmpXE7c7Kdf+oYTps2nqNOaGPvfd8IUOPGiF+NMjx+abffMZbzL5gWNGanatS/lDKy5gSVW4NlZk+a2SQzmwT8A7AZuK7SuLHnfKtGTrnYv0Ps8SHf3Jb1kZewwXpYO5gOPGNmz1caKPacb9XIKRf7d4g9ft5qXn9L7hJm2UKr1qD7ycDcKpXlnMtbow66S2oGjgfmdfPzGZ1ZYbexpWS82HO+VSOnXOzfIfb4eauH+sss0xZaNS4JjwWWmNlOb2WUm0g19pxv1cgpF/t3iD1+3uqi/jUaw6rGJeEpBLwcjD3nWzVyysX+HWKPD/nmtqx5XkIDAiWhkPRr4CPAOjPbv+T+Oecl7Ae8AIw1s5K3MfLOS+hcCDEvLxMiL+HAfnvZIRNOz7Rv66LvF81LKOlI4DVgdpYGK++8hJuAd+VZhnOuBjrCdLHM7B5Jo7Pu74/mOOfKU94l4RBJi7q8n2Vms3a1aG+wnHNlK+MO4PpoUtU75xqUr9bgnItD7RKpRrlag3Ouhjqz5mTZSpA0F3gAGC9plaR/Kba/97Ccc2ULNYvdzE4pZ39vsJxz5fMxLFCf3hT2y29SXgz53lz92+3/jgzo8AbLOReF2g26e4PlnCufN1jOuSgY0B7o6ecyeYPlnCuTgXmD5ZyLhV8SOuei4HcJnXNR8Udzsos9SaUnUvX49R6/pEZM8yXpHEmPSVomaa6kIOu4xpyk0hOpevx6j1+SGbS3Z9sCyzPz80jgq8CUdOnTJpJ0XxWLOUmlJ1L1+PUeP5NG7GGRjJH1kdQD6Ausybm8ijVCEs/Yv4PHr238TBqtwTKz1cD/JUlC8SLwP2bWuuN+XfMSbt2+Oa/qOOeCseQuYZYtsDwvCfcETgDGAHsB/SR9esf9uuYlbO7RN6/qZNYISTxj/w4ev7bxSzIw68i0hZbnJeE/As+a2ctmtg2YDxyWY3lBNEISz9i/g8evbfxM2juybYHlOQ/rBeAQSX2B14HpwKLih2QTc5JKT6Tq8es9fklmwdJ8lSvvRKo/AD4JbAf+AnzRzLZ0t//AvnvZIfudllt9dvt1jNxuL0gi1aYhdmi/f8q0720bf1M0kWq58k6keiFwYZ5lOOeqz2rUw/JHc5xzZfIF/JxzsfCHn51zsTDAcnjsJosoH352ztWQpQv4ZdlKkHSMpCclPS3p26X29x6Wc65sFuCSUFIT8AvgA8Aq4GFJN5pZt7fzvYflnCtfmB7WVOBpM1thZluB35E8HdOtXOdhlUvSy8DzZRwyBFifU3U8fuPHr0YZ9RZ/HzMbWkmBkm5Ny82iN9B17ZtZZjYrjfNx4Bgz+2L6/jPAwWb25e6C1dUlYbknUtKikJPSPP7uFb8aZcQef2fM7JhqlteVXxI652plNdDS5f2o9LNueYPlnKuVh4F9JY2R1EyywOeNxQ6oq0vCXTDL43v8Oi8j9vi5MbPtkr4M3EayIvGvzeyxYsfU1aC7c84V45eEzrloeIPlnItGlA1WudP5dyH+ryWtk7QsdOw0foukuyQ9nqZBOztw/N6SHpL01zT+D0LG71JOk6S/SPpDDrGfk/SopKWSgiz8uEP8QZKulfSEpOWSDg0Ye3xa785tg6SZoeKnZeSSQq/umVlUG8ng3DPAWKAZ+CswIXAZRwKTgWU5fYcRwOT0dX/gbyG/AyBgj/R1T+BB4JAcvsfXgDnAH3KI/RwwJMe/o9+SLChJ+nc0KKdymoC1JBM2Q8UcCTwL9EnfXwN8Lq9zVU9bjD2ssqfzl8vM7gFeDRlzh/gvmtmS9PVGYDnJH2Go+GZmr6Vve6Zb0LsrkkYBHwYuCxm3GiQNJPmf0uUAZrbVzNpyKm468IyZlfMERxbRpdALIcYGaySwssv7VQT8j73aJI0GDiLpBYWM2yRpKbAOuN3MgsYHLga+CeS19KQBrZIWS5oROPYY4GXgivSS9jJJ/QKX0elkYG7IgJYxhV4jirHBahiS9gB+D8w0sw0hY5tZu5lNIpk9PFXS/qFiS/oIsM7MFoeKuRNHmNlk4FjgLElHBozdg+SS/1IzOwjYBOQxFtoMHA/MCxw3Uwq9RhRjg1X2dP56JKknSWN1tZnNz6uc9FLnLiDk81+HA8dLeo7kkvxoSVcFjN/Zi8DM1gHXkQwFhLIKWNWl13ktSQMW2rHAEjN7KXDcKFPohRBjg1X2dP56I0kk4yfLzeynOcQfKmlQ+roPyXpDT4SKb2bnmdkoMxtNcv7vNLNg/4eX1E9S/87XwAeBYHdszWwtsFLS+PSj6UAeKZVOIfDlYOrNFHrp39J0knHQhhfdozm2C9P5yyVpLjANGCJpFXChmV0esIjDgc8Aj6bjTADfMbObA8UfAfw2XSCtAFxjZsGnHuRoOHBd8t8iPYA5ZnZr4DK+Alyd/k9vBfD5kMHThvYDwOkh4wKY2YOSrgWW8FYKvWgf0SmHP5rjnItGjJeEzrndlDdYzrloeIPlnIuGN1jOuWh4g+Wci4Y3WBGR1J4+/b9M0jxJfSuI9Zs0awnpoykTiuw7TVLZExPTFRfekV2lu8932Oe1Yj/fyf7fl3RuuXV0cfEGKy6vm9kkM9sf2Aqc0fWH6YOwZTOzL1qR5JUkc9J2i5nUrr55gxWvBcC4tPezQNKNwOPpQ8//R9LDkh6RdDoks+slXZKuI3YHMKwzkKS7JU1JXx8jaUm6ltaf0oezzwDOSXt3709n0v8+LeNhSYenx75LUmu6TtNlJMvcFCXp+vQB58d2fMhZ0r+nn/9J0tD0s/dIujU9ZoGk/YKcTReHWq9v41v2DXgt/bcHcAPwJZLezyZgTPqzGcD56etewCKSh2RPBG4neTpgL6AN+Hi6393AFGAoyUoYnbEGp/9+Hzi3Sz3mkDycDLA3ySNGAD8Hvpe+/jDJigvvWNOKLmtddSmjD8njN+9K3xtwavr6e8Al6es/Afumrw8meSzoHXX0rTG36B7N2c316fIozwKS5xEPAx4ys2fTzz8ITOwcnwIGAvuSrP8018zagTWS7txJ/EOAezpjmVl3a4L9IzAhfXQGYEC68sSRJA0jZvZHSX/P8J2+Kulj6euWtK6vkCxb89/p51cB89MyDgPmdSm7V4YyXIPwBisur1uyZMyb0v9wN3X9CPiKmd22w37HBaxHgWQF064pyOnSiGQiaRpJ43eomW2WdDdJavOdsbTcth3Pgdt9+BhW47kN+FK6fA2S3ps+iHsP8Ml0jGsEcNROjl0IHClpTHrs4PTzjSRLOXdqJXl4mHS/SenLe4BPpZ8dC+xZoq4Dgb+njdV+JD28TgWgs5f4KeBeS9YMe1bSJ9IyJOnAEmW4BuINVuO5jGSplCVKkmj8iqQnfR3wVPqz2cADOx5oZi+TjIHNl/RX3rokuwn4WOegO/BVYEo6qP84b92t/AFJg/cYyaXhCyXqeivQQ9Jy4F9JGsxOm0gWHlwGHA38MP38VOBf0vo9RuDlsV1989UanHPR8B6Wcy4a3mA556LhDZZzLhreYDnnouENlnMuGt5gOeei4Q2Wcy4a/x+F76vQ8dLG4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_predictions4 = model4.predict(val_images)\n",
    "\n",
    "y_true = [np.argmax(row) for row in val_labels]\n",
    "y_pred4 = [np.argmax(row) for row in model_predictions4]\n",
    "\n",
    "matrix = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred4))\n",
    "\n",
    "matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_16 (Sequential)  (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intercept_layer = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in intercept_layer.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "imput_layer = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "    \n",
    "model5 = keras.Sequential([\n",
    "    imput_layer,\n",
    "    intercept_layer,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model5.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Adam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 1616s 3s/step - loss: 1.3799 - accuracy: 0.5449 - val_loss: 1.1667 - val_accuracy: 0.4688\n",
      "INFO:tensorflow:Assets written to: model/m5/assets\n"
     ]
    }
   ],
   "source": [
    "history5 = model5.fit(train_generator,\n",
    "                    epochs=1,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    verbose=1)\n",
    "\n",
    "model5.save(f\"model/m5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_18 (Sequential)  (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_9   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intercept_layer = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in intercept_layer.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "imput_layer = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "    \n",
    "model6 = keras.Sequential([\n",
    "    imput_layer,\n",
    "    intercept_layer,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model6.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Nadam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 [==============================] - 1692s 4s/step - loss: 1.3812 - accuracy: 0.5421 - val_loss: 1.1540 - val_accuracy: 0.5938\n",
      "INFO:tensorflow:Assets written to: model/m5/assets\n"
     ]
    }
   ],
   "source": [
    "history6 = model6.fit(train_generator,\n",
    "                    epochs=1,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    verbose=1)\n",
    "\n",
    "model6.save(f\"model/m5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_3 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, None, None, 2048)  21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 12)                24588     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,827,372\n",
      "Trainable params: 24,588\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "234/466 [==============>...............] - ETA: 13:29 - loss: 1.4757 - accuracy: 0.5096"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m model7\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m                metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     22\u001b[0m model7\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 24\u001b[0m history7 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel7\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m model7\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/m5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Desktop/uni/sandbox/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "intercept_layer = inception_v3.InceptionV3(weights ='imagenet', include_top = False)\n",
    "for layer in intercept_layer.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "imput_layer = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=shape),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "model7 = keras.Sequential([\n",
    "    imput_layer,\n",
    "    intercept_layer,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(classes_amount, kernel_initializer = 'uniform', activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model7.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'Adam',\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model7.summary()\n",
    "\n",
    "history7 = model7.fit(train_generator,\n",
    "                    epochs=1,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    verbose=1)\n",
    "\n",
    "model7.save(f\"model/m5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
